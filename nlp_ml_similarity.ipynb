{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "331ba65c",
   "metadata": {},
   "source": [
    "Анализ схожести научных текстов с помощью методов естественной обработки языка и машинного обучения\n",
    "Береза Анастасия // Учебная группа о.ИЗДтс 23.2/Б3-22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f2ca2d",
   "metadata": {},
   "source": [
    "# Обработка естественного языка"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a7e5dd",
   "metadata": {},
   "source": [
    "## Установка зависимостей\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b54c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# При первом запуске раскомментировать строку ниже\n",
    "%pip install kagglehub pandas numpy scikit-learn joblib nltk pymorphy3 matplotlib python-docx pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ebde9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu130"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c2bacc",
   "metadata": {},
   "source": [
    "## Импорт библиотек\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7507c9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import shutil\n",
    "import re\n",
    "import random\n",
    "import pymorphy3\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import kagglehub\n",
    "import joblib\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import download\n",
    "\n",
    "from itertools import tee\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n",
    "from docx import Document\n",
    "from datetime import datetime\n",
    "from tkinter import Tk\n",
    "from tkinter.filedialog import askopenfilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec766fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Используемое устройство:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f1b5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    # Для GPU\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "    # Оотключение недетерминированных алгоритмов\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Seed fixed: {seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463f08a9",
   "metadata": {},
   "source": [
    "## Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fa139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"ergkerg/russian-scientific-articles\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# Копирование в папку проекта\n",
    "\n",
    "destination_path = os.path.join(os.getcwd(), \"russian-scientific-articles\")\n",
    "shutil.copytree(path, destination_path, dirs_exist_ok=True)\n",
    "\n",
    "print(\"Файлы датасета перенесены в:\", destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f02225b",
   "metadata": {},
   "source": [
    "## Сбор датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae4c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сбор датасета из txt-файлов\n",
    "\n",
    "rows = []\n",
    "for root in [\"russian-scientific-articles/data_3_1\", \"russian-scientific-articles/data_3\"]:\n",
    "     for p in Path(root).rglob(\"*.txt\"):\n",
    "        rows.append({\n",
    "              \"category\": p.parent.name,\n",
    "              \"file\": str(p),\n",
    "              \"text\": p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbf2997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение в файле\n",
    "\n",
    "data_dir = Path(\"data\")\n",
    "data_dir.mkdir(exist_ok=True)\n",
    "\n",
    "df_path = data_dir / \"df.parquet\"\n",
    "df.to_parquet(df_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b4162",
   "metadata": {},
   "source": [
    "## Загрузка собранного датасета. Анализ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfd3b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка из файла\n",
    "\n",
    "df = pd.read_parquet(\"data/df.parquet\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50d2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b27d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608ff11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af1c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(['text',], inplace=True)\n",
    "df['text'].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25ac7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Извлечение слов из всех статей\n",
    "\n",
    "def raw_tokens(text):\n",
    "    return re.findall(r\"\\w+\", str(text).lower())\n",
    "\n",
    "tokens = []\n",
    "for article in df[\"text\"].dropna():\n",
    "    tokens.extend(raw_tokens(article))\n",
    "\n",
    "# Гистограмма для топ-100 слов\n",
    "\n",
    "top_n = 100\n",
    "freq = Counter(tokens).most_common(top_n)\n",
    "words, counts = zip(*freq)\n",
    "\n",
    "plt.figure(figsize=(25,4))\n",
    "plt.bar(words, counts)\n",
    "plt.xticks(rotation=70, ha=\"right\")\n",
    "plt.title(\"Топ-100 слов\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Топ-100 слов:\\n\")\n",
    "for i, (w, c) in enumerate(zip(words, counts), start=1):\n",
    "    print(f\"{i:3}. {w:<20} {c}\")\n",
    "\n",
    "# Топ пар из 2 слов \n",
    "\n",
    "def bigrams(seq):\n",
    "    a, b = tee(seq)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "\n",
    "bigram_freq = Counter(bigrams(tokens)).most_common(50)\n",
    "print(\"\\nТоп 50-биграмм\")\n",
    "for (w1, w2), c in bigram_freq:\n",
    "    print(f\"{w1} {w2}: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92895a69",
   "metadata": {},
   "source": [
    "## Очистка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ff843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стоп-слова\n",
    "\n",
    "download(\"stopwords\", quiet=True)\n",
    "stop_ru = set(stopwords.words(\"russian\"))\n",
    "stop_en = set(stopwords.words(\"english\"))\n",
    "custom_stop = {\n",
    "    # указатели, ссылки\n",
    "    \"doi\",\"org\",\"orcid\",\"http\",\"https\",\"url\",\"удк\", \n",
    "    # предлоги\n",
    "    \"в\",\"и\",\"с\",\"на\",\"по\",\"для\",\"что\",\"как\",\"от\",\"из\",\"при\",\"но\",\"же\",\"у\",\"о\",\"к\",\n",
    "    # предлоги на англ\n",
    "    \"the\",\"of\",\"in\",\"on\",\"and\",\"a\",\"to\",\"for\",\"is\",\"are\",\n",
    "    # одиночные буквы\n",
    "    \"к\",\"р\",\"п\",\"г\",\"т\",\"е\",\"а\",\"м\",\"н\",\"л\",\"у\",\n",
    "    # цифры\n",
    "    \"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\"\n",
    "}\n",
    "custom_bigram_stop = {\n",
    "    # служебные\n",
    "    \"и в\",\"а также\",\"при этом\",\"а в\",\"на основе\",\"в том\",\"и др\",\"в качестве\",\"так и\",\"что в\",\"не только\",\n",
    "    \"но и\", \"таким образом\",\"том что\",\"том числе\",\"в рамках\",\"в результате\",\"в россии\",\"в виде\",\"в этом\",\n",
    "    \"с помощью\",\"на рис\",\n",
    "    # пары чисел\n",
    "    \"1 1\",\"1 2\",\"1 0\",\"2 1\",\"2 2\",\"2 3\",\"2 0\",\n",
    "    \"0 0\",\"0 1\",\"0 5\",\"а а\",\"в в\",\"в а\",\n",
    "}\n",
    "stop_all = stop_ru | stop_en | custom_stop | custom_bigram_stop\n",
    "\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "english_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Учитывать ли латиницу\n",
    "\n",
    "keep_english = True\n",
    "min_len = 3\n",
    "\n",
    "# Сохранять кэш слов\n",
    "\n",
    "lemma_cache: Dict[str, str] = {}\n",
    "stem_cache: Dict[str, str] = {}\n",
    "\n",
    "# Наборы шаблон-выражений\n",
    "\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\", re.IGNORECASE)\n",
    "EMAIL_RE = re.compile(r\"\\b[\\w.+-]+@[\\w-]+\\.[\\w.-]+\\b\")\n",
    "DOI_RE = re.compile(r\"\\b10\\.\\d{4,9}/[-._;()/:A-Za-z0-9]+\\b\", re.IGNORECASE)\n",
    "ORCID_RE = re.compile(r\"\\b\\d{4}-\\d{4}-\\d{4}-\\d{3}[0-9X]\\b\")\n",
    "NUMBER_RE = re.compile(r\"\\b\\d[\\d.,/%:-]*\\d\\b\")\n",
    "TOKEN_RE = re.compile(r\"[A-Za-z\\u0400-\\u04FF]+\")\n",
    "CYRILLIC_RE = re.compile(r\"[\\u0400-\\u04FF]\")\n",
    "LATIN_RE = re.compile(r\"[A-Za-z]\")\n",
    "\n",
    "# Функция для очистки по шаблонам\n",
    "\n",
    "def strip_noise(text: str) -> str:\n",
    "    text = text.replace(\"\\u00a0\", \" \" ).replace(\"\\ufeff\", \" \" )\n",
    "    text = URL_RE.sub(\" \" , text)\n",
    "    text = EMAIL_RE.sub(\" \" , text)\n",
    "    text = DOI_RE.sub(\" \" , text)\n",
    "    text = ORCID_RE.sub(\" \" , text)\n",
    "    text = NUMBER_RE.sub(\" \" , text)\n",
    "    return text\n",
    "\n",
    "# Функция предобработки текста \n",
    "\n",
    "def clean_text(raw: str) -> str:\n",
    "    text = strip_noise(raw.lower())\n",
    "    lemmas: list[str] = []\n",
    "    for token in TOKEN_RE.findall(text):\n",
    "        if len(token) < min_len:\n",
    "            continue\n",
    "        has_cyr = bool(CYRILLIC_RE.search(token))\n",
    "        has_lat = bool(LATIN_RE.search(token))\n",
    "        if has_cyr and has_lat:\n",
    "            continue\n",
    "        if has_cyr:\n",
    "            lemma = lemma_cache.get(token)\n",
    "            if lemma is None:\n",
    "                lemma = morph.parse(token)[0].normal_form\n",
    "                lemma_cache[token] = lemma\n",
    "            if len(lemma) >= min_len and lemma not in stop_all:\n",
    "                lemmas.append(lemma)\n",
    "        elif has_lat and keep_english:\n",
    "            stem = stem_cache.get(token)\n",
    "            if stem is None:\n",
    "                stem = english_stemmer.stem(token)\n",
    "                stem_cache[token] = stem\n",
    "            if len(stem) >= min_len and stem not in stop_all:\n",
    "                lemmas.append(stem)\n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7f0df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предобработка датасета с помощью функции \n",
    "\n",
    "clean_df = (\n",
    "    df.assign(clean_text=df[\"text\"].map(clean_text))\n",
    "      .loc[lambda d: d[\"clean_text\"] != \"\"]\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "clean_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6929fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b4f3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df[\"clean_text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5550cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение в файл очищенный набор данных\n",
    "clean_df.to_json(\"cleaned_dataset.jsonl\", orient=\"records\", lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be21623d",
   "metadata": {},
   "source": [
    "### Визуализация частоты использования очищенных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3818b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = clean_df[\"clean_text\"].str.split().explode()\n",
    "freq = tokens.value_counts().head(30)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "freq.sort_values().plot(kind=\"barh\")\n",
    "plt.xlabel(\"Частота\")\n",
    "plt.ylabel(\"Слова\")\n",
    "plt.title(\"Топ-30 самых частых лемм после очистки\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab9c966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Количество статей по рубрикам\n",
    "category_counts = (clean_df[\"category\"].value_counts().sort_values())\n",
    "\n",
    "# Вывод столбчатой диаграммы\n",
    "fig, ax = plt.subplots(figsize=(25, 5))\n",
    "ax.bar(category_counts.index.astype(str), category_counts.values)\n",
    "ax.set_xlabel(\"Рубрика\")\n",
    "ax.set_ylabel(\"Количество статей\")\n",
    "ax.set_title(\"Распределение статей по рубрикам\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d42d53",
   "metadata": {},
   "source": [
    "# Машинное обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cdccff",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95bfcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка набора данных из файла\n",
    "clean_df = pd.read_json(\"cleaned_dataset.jsonl\", lines=True)\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee84059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделение на обучающую и валидационную выборку\n",
    "\n",
    "train_df, test_df = train_test_split(clean_df, test_size=0.2, stratify=clean_df[\"category\"], random_state = 42)\n",
    "\n",
    "print(\"Размер train:\", train_df.shape)\n",
    "print(\"Размер test:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdae0ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Векторизация TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), lowercase=False, min_df=2)\n",
    "\n",
    "train_texts = train_df[\"clean_text\"].tolist()\n",
    "test_texts = test_df[\"clean_text\"].tolist()\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "X_test = vectorizer.transform(test_texts)\n",
    "\n",
    "y_train = train_df[\"category\"].astype(str).to_numpy()\n",
    "y_test = test_df[\"category\"].astype(str).to_numpy()\n",
    "\n",
    "print(\"Матрица X_train:\", X_train.shape)\n",
    "print(\"Матрица X_test:\", X_test.shape)\n",
    "\n",
    "# Сохранение векторизатора для будущего использования\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d727f256",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NNZ train:\", X_train.nnz)\n",
    "print(\"Среднее число ненулевых признаков на документ:\",\n",
    "      X_train.nnz / X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dea367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция создания пар текстов\n",
    "\n",
    "def make_pairs(X, y, n_pos=3, n_neg=3, random_state= 42):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    label2idx = defaultdict(list)\n",
    "    for idx, label in enumerate(y):\n",
    "        label2idx[label].append(idx)\n",
    "\n",
    "    all_index = np.arange(len(y))\n",
    "    pair_i = []\n",
    "    pair_j = []\n",
    "    pair_labels = []\n",
    "\n",
    "    for label, idxs in label2idx.items():\n",
    "        idxs = np.asarray(idxs)\n",
    "        other_index = np.setdiff1d(all_index, idxs)\n",
    "\n",
    "        for i in idxs:\n",
    "            # положительные пары (та же категория)\n",
    "            if len(idxs) > 1:\n",
    "                pos_candidates = idxs[idxs != i]\n",
    "                n_sample_pos = min(n_pos, len(pos_candidates))\n",
    "                pos = rng.choice(pos_candidates, size=n_sample_pos, replace=False)\n",
    "                for j in pos:\n",
    "                    pair_i.append(i)\n",
    "                    pair_j.append(j)\n",
    "                    pair_labels.append(1)\n",
    "\n",
    "            # отрицательные пары (другая категория)\n",
    "            if len(other_index) > 0:\n",
    "                n_sample_neg = min(n_neg, len(other_index))\n",
    "                neg = rng.choice(other_index, size=n_sample_neg, replace=False)\n",
    "                for j in neg:\n",
    "                    pair_i.append(i)\n",
    "                    pair_j.append(j)\n",
    "                    pair_labels.append(0)\n",
    "\n",
    "    pair_i = np.asarray(pair_i)\n",
    "    pair_j = np.asarray(pair_j)\n",
    "    pair_labels = np.asarray(pair_labels, dtype=int)\n",
    "\n",
    "    print(f\"Сформировано пар: {len(pair_labels)} \"\n",
    "          f\"(положительных: {(pair_labels == 1).sum()}, отрицательных: {(pair_labels == 0).sum()})\")\n",
    "\n",
    "    return pair_i, pair_j, pair_labels\n",
    "\n",
    "\n",
    "# Формирование обучающих и тестовых пар\n",
    "train_idx_i, train_idx_j, y_train_pairs = make_pairs(X_train, y_train, n_pos=3, n_neg=3, random_state=42)\n",
    "test_idx_i, test_idx_j, y_test_pairs = make_pairs(X_test, y_test, n_pos=3, n_neg=3, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecef712",
   "metadata": {},
   "source": [
    "## Проектирование модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0c4390",
   "metadata": {},
   "source": [
    "### Модель CosineMLP\n",
    "Принимает один признак - косинус между векторами пары текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6960d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Формирование данных в виде косинуса\n",
    "\n",
    "def pair_cosine_features(X, idx_i, idx_j):\n",
    "    idx_i = np.asarray(idx_i)\n",
    "    idx_j = np.asarray(idx_j)\n",
    "    sims = []\n",
    "    for a, b in zip(idx_i, idx_j):\n",
    "        Xi = X[a]\n",
    "        Xj = X[b]\n",
    "        sims.append([cosine_similarity(Xi, Xj)[0, 0]])\n",
    "    return np.array(sims, dtype=\"float32\")\n",
    "\n",
    "# Генерация признаков для пар\n",
    "\n",
    "X_train_pairs = pair_cosine_features(X_train, train_idx_i, train_idx_j)\n",
    "X_test_pairs  = pair_cosine_features(X_test,  test_idx_i,  test_idx_j)\n",
    "\n",
    "print(\"Форма X_train_pairs:\", X_train_pairs.shape)\n",
    "print(\"Форма X_test_pairs :\", X_test_pairs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cbde4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тензоры и dataloader для модели\n",
    "\n",
    "X_train_cos_t = torch.from_numpy(X_train_pairs)\n",
    "y_train_cos_t = torch.from_numpy(y_train_pairs.astype(\"float32\")).view(-1, 1)\n",
    "\n",
    "X_test_cos_t  = torch.from_numpy(X_test_pairs)\n",
    "y_test_cos_t  = torch.from_numpy(y_test_pairs.astype(\"float32\")).view(-1, 1)\n",
    "\n",
    "cos_train_ds = TensorDataset(X_train_cos_t, y_train_cos_t)\n",
    "cos_test_ds  = TensorDataset(X_test_cos_t,  y_test_cos_t)\n",
    "\n",
    "cos_batch_size = 512\n",
    "\n",
    "cos_train_loader = DataLoader(cos_train_ds, batch_size=cos_batch_size, shuffle=True)\n",
    "cos_test_loader  = DataLoader(cos_test_ds,  batch_size=cos_batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48c7751",
   "metadata": {},
   "source": [
    "#### Определение архитектуры модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a255d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(1, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 8),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(8, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b07e159",
   "metadata": {},
   "source": [
    "#### Функция обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dd00db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cosine_model(train_loader, test_loader, device, num_epochs=10):\n",
    "    model = CosineMLP().to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": [],\n",
    "        \"test_auc\": []\n",
    "    }\n",
    "\n",
    "    best_auc = -1.0\n",
    "    best_state = None\n",
    "    patience = 3\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Обучение\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Тестирование\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        all_logits = []\n",
    "        all_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                test_loss += loss.item() * xb.size(0)\n",
    "\n",
    "                all_logits.append(logits.cpu())\n",
    "                all_targets.append(yb.cpu())\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "\n",
    "        all_logits = torch.cat(all_logits, dim=0)\n",
    "        all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "        proba = torch.sigmoid(all_logits).numpy().reshape(-1)\n",
    "        true  = all_targets.numpy().reshape(-1)\n",
    "        pred  = (proba >= 0.5).astype(\"int32\")\n",
    "\n",
    "        acc = accuracy_score(true, pred)\n",
    "        auc = roc_auc_score(true, proba)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"test_loss\"].append(test_loss)\n",
    "        history[\"test_acc\"].append(acc)\n",
    "        history[\"test_auc\"].append(auc)\n",
    "\n",
    "        print(f\"CosineMLP: Эпоха {epoch}: \"\n",
    "              f\"train_loss={train_loss:.4f}  test_loss={test_loss:.4f}  \"\n",
    "              f\"Точность={acc:.4f}  AUC={auc:.4f}\")\n",
    "\n",
    "        # Early Stop\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_state = model.state_dict()\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"AUC не улучшался {patience} эпох(и). Остановка обучения.\")\n",
    "                break\n",
    "\n",
    "    # Загружаются веса с лучшей эпохи\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# Выполнение\n",
    "set_seed(42)\n",
    "\n",
    "cos_model, hist_cos = train_cosine_model(cos_train_loader, cos_test_loader, device, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88177c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(hist_cos[\"train_loss\"]) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochs, hist_cos[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(epochs, hist_cos[\"test_loss\"], label=\"test_loss\")\n",
    "plt.xlabel(\"Эпоха\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"CosineMLP: Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs, hist_cos[\"test_acc\"], label=\"test_acc\")\n",
    "plt.xlabel(\"Эпоха\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"CosineMLP: Accuracy\")\n",
    "\n",
    "# AUC\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs, hist_cos[\"test_auc\"], label=\"test_auc\")\n",
    "plt.xlabel(\"Эпоха\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"CosineMLP: AUC\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950ca01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение модели\n",
    "\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "torch.save(cos_model.state_dict(), \"models/cosine_mlp.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41b361",
   "metadata": {},
   "source": [
    "### Модель PairMLP\n",
    "Принимает два признака - векторы пары текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e478c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Формирование признаков: два вектора для каждой пары\n",
    "\n",
    "def pair_vector_features(X, idx_i, idx_j):\n",
    "    idx_i = np.asarray(idx_i)\n",
    "    idx_j = np.asarray(idx_j)\n",
    "    X1 = X[idx_i].toarray().astype(\"float32\")\n",
    "    X2 = X[idx_j].toarray().astype(\"float32\")\n",
    "\n",
    "    return X1, X2\n",
    "\n",
    "# Генерация признаков для пар (train / test)\n",
    "\n",
    "X_train_1, X_train_2 = pair_vector_features(X_train, train_idx_i, train_idx_j)\n",
    "X_test_1, X_test_2 = pair_vector_features(X_test, test_idx_i, test_idx_j)\n",
    "\n",
    "print(\"Форма X_train_1:\", X_train_1.shape)\n",
    "print(\"Форма X_train_2:\", X_train_2.shape)\n",
    "print(\"Форма X_test_1:\", X_test_1.shape)\n",
    "print(\"Форма X_test_2:\", X_test_2.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68d8125",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairVectorDataset(Dataset):\n",
    "    def __init__(self, X1, X2, y):\n",
    "        self.X1 = torch.from_numpy(X1)\n",
    "        self.X2 = torch.from_numpy(X2)\n",
    "        self.y = torch.from_numpy(y.astype(\"float32\")).view(-1, 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X1[idx], self.X2[idx], self.y[idx]\n",
    "\n",
    "\n",
    "pair_batch_size = 512\n",
    "\n",
    "train_pair_ds = PairVectorDataset(X_train_1, X_train_2, y_train_pairs)\n",
    "test_pair_ds= PairVectorDataset(X_test_1,X_test_2,y_test_pairs)\n",
    "\n",
    "train_pair_loader = DataLoader(train_pair_ds, batch_size=pair_batch_size, shuffle=True)\n",
    "test_pair_loader = DataLoader(test_pair_ds, batch_size=pair_batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5a156e",
   "metadata": {},
   "source": [
    "#### Определение архитектуры модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c7fbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256, dropout_p=0.3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4eba8d",
   "metadata": {},
   "source": [
    "#### Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86fd941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сколько входных признаков\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "print(\"Размер TF-IDF признакового пространства:\", input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5322217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pair_mlp(train_loader, test_loader, device, input_dim, num_epochs=10):\n",
    "    model = PairMLP(input_dim=input_dim).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": [],\n",
    "        \"test_auc\": []\n",
    "    }\n",
    "\n",
    "    best_auc = -1.0\n",
    "    best_state = None\n",
    "    patience = 3\n",
    "    no_improve = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        # Обучение\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        for xb1, xb2, yb in train_loader:\n",
    "            xb1 = xb1.to(device)\n",
    "            xb2 = xb2.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(xb1, xb2)\n",
    "            loss = criterion(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item() * xb1.size(0)\n",
    "\n",
    "        train_loss = total_train_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Тестирование\n",
    "        model.eval()\n",
    "        total_test_loss = 0.0\n",
    "        all_logits = []\n",
    "        all_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb1, xb2, yb in test_loader:\n",
    "                xb1 = xb1.to(device)\n",
    "                xb2 = xb2.to(device)\n",
    "                yb = yb.to(device)\n",
    "\n",
    "                logits = model(xb1, xb2)\n",
    "                loss = criterion(logits, yb)\n",
    "                total_test_loss += loss.item() * xb1.size(0)\n",
    "\n",
    "                all_logits.append(logits.cpu())\n",
    "                all_targets.append(yb.cpu())\n",
    "\n",
    "        test_loss = total_test_loss / len(test_loader.dataset)\n",
    "\n",
    "        all_logits = torch.cat(all_logits, dim=0)\n",
    "        all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "        proba = torch.sigmoid(all_logits).numpy().reshape(-1)\n",
    "        true = all_targets.numpy().reshape(-1)\n",
    "        pred = (proba >= 0.5).astype(\"int32\")\n",
    "\n",
    "        acc = accuracy_score(true, pred)\n",
    "        auc = roc_auc_score(true, proba)\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"test_loss\"].append(test_loss)\n",
    "        history[\"test_acc\"].append(acc)\n",
    "        history[\"test_auc\"].append(auc)\n",
    "\n",
    "        print(\n",
    "            f\"PairMLP: Эпоха {epoch}: \"\n",
    "            f\"train_loss={train_loss:.4f} test_loss={test_loss:.4f} \"\n",
    "            f\"Точность={acc:.4f} AUC={auc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # early stopping по AUC\n",
    "        if auc > best_auc:\n",
    "            best_auc = auc\n",
    "            best_state = model.state_dict()\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "            if no_improve >= patience:\n",
    "                print(f\"AUC не улучшался {patience} эпох(и). Остановка обучения.\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return model, history\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "pair_model, hist_pair = train_pair_mlp(train_pair_loader, test_pair_loader, device, input_dim=input_dim, num_epochs=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749662a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(hist_pair[\"train_loss\"]) + 1)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Loss\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(epochs, hist_pair[\"train_loss\"], label=\"train_loss\")\n",
    "plt.plot(epochs, hist_pair[\"test_loss\"], label=\"test_loss\")\n",
    "plt.xlabel(\"Эпоха\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"PairMLP: Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Accuracy\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(epochs, hist_pair[\"test_acc\"], label=\"test_acc\")\n",
    "plt.xlabel(\"Эпоха\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"PairMLP: Accuracy\")\n",
    "\n",
    "# AUC\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(epochs, hist_pair[\"test_auc\"], label=\"test_auc\")\n",
    "plt.xlabel(\"Эпоха\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.title(\"PairMLP: AUC\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689e8aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "torch.save(pair_model.state_dict(), \"models/pair_mlp.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bc3111",
   "metadata": {},
   "source": [
    "# Тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bb8823",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Используемое устройство:\", device)\n",
    "\n",
    "# Загрузка TF-IDF векторизатора\n",
    "vectorizer = joblib.load(\"tfidf_vectorizer.joblib\")\n",
    "\n",
    "# Определяем размерность TF-IDF-признаков\n",
    "tfidf_dim = vectorizer.transform([\"тестовая строка\"]).shape[1]\n",
    "print(\"Размерность TF-IDF:\", tfidf_dim)\n",
    "\n",
    "# Загрузка модели CosineMLP\n",
    "cos_model = CosineMLP().to(device)\n",
    "cos_model.load_state_dict(torch.load(\"models/cosine_mlp.pt\", map_location=device))\n",
    "cos_model.eval()\n",
    "\n",
    "# Загрузка модели PairMLP\n",
    "\n",
    "pair_model = PairMLP(input_dim=tfidf_dim).to(device)\n",
    "pair_model.load_state_dict(torch.load(\"models/pair_mlp.pt\", map_location=device))\n",
    "pair_model.eval()\n",
    "\n",
    "print(\"Модели загружены.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec17334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция  подготовки пары текстов для CosineMLP\n",
    "\n",
    "def prepare_tfidf_pair_cosine(text1: str, text2: str):\n",
    "    clean1 = clean_text(text1)\n",
    "    clean2 = clean_text(text2)\n",
    "\n",
    "    # Проверка пустого текста\n",
    "    if clean1.strip() == \"\" or clean2.strip() == \"\":\n",
    "        print(\"Внимание: один из текстов после очистки оказался пустым.\")\n",
    "    \n",
    "    X_pair = vectorizer.transform([clean1, clean2])\n",
    "    x1 = X_pair[0]\n",
    "    x2 = X_pair[1]\n",
    "    return x1, x2, clean1, clean2\n",
    "\n",
    "# Функция  подготовки пары текстов для PairMLP\n",
    "\n",
    "def prepare_tfidf_pair_twovectors(text1: str, text2: str):\n",
    "    clean1 = clean_text(text1)\n",
    "    clean2 = clean_text(text2)\n",
    "\n",
    "    if clean1.strip() == \"\" or clean2.strip() == \"\":\n",
    "        print(\"Внимание: один из текстов после очистки оказался пустым.\")\n",
    "        \n",
    "    X_pair = vectorizer.transform([clean1, clean2]).toarray().astype(\"float32\")\n",
    "    x1 = torch.from_numpy(X_pair[0:1]).to(device)\n",
    "    x2 = torch.from_numpy(X_pair[1:1+1]).to(device)\n",
    "\n",
    "    return x1, x2, clean1, clean2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31db8ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Использование cosineMLP для анализа текстов\n",
    "\n",
    "def predict_with_cosine_mlp(text1: str, text2: str, threshold: float = 0.5):\n",
    "\n",
    "    x1, x2, clean1, clean2 = prepare_tfidf_pair_cosine(text1, text2)\n",
    "\n",
    "    # Косинусное сходство между TF-IDF векторами\n",
    "    cos = float(cosine_similarity(x1, x2)[0, 0])\n",
    "\n",
    "    # Преобразуем в тензор формы (1, 1)\n",
    "    inp = torch.tensor([[cos]], dtype=torch.float32, device=device)\n",
    "\n",
    "    cos_model.eval()\n",
    "    with torch.no_grad():\n",
    "        logit = cos_model(inp)\n",
    "        prob = torch.sigmoid(logit).item()\n",
    "\n",
    "    label = int(prob >= threshold)\n",
    "    return prob, label, cos\n",
    "\n",
    "# Использование pairMLP для анализа\n",
    "\n",
    "def predict_with_pair_mlp(text1: str, text2: str, threshold: float = 0.5):\n",
    "    x1_t, x2_t, clean1, clean2 = prepare_tfidf_pair_twovectors(text1, text2)\n",
    "\n",
    "    pair_model.eval()\n",
    "    with torch.no_grad():\n",
    "        logit = pair_model(x1_t, x2_t)\n",
    "        prob = torch.sigmoid(logit).item()\n",
    "\n",
    "    label = int(prob >= threshold)\n",
    "    return prob, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d25ab92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Открыт в google.colab или нет\n",
    "\n",
    "def in_colab() -> bool:\n",
    "    try:\n",
    "        import google.colab  # noqa: F401\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Загрузка текста из пользовательского файла\n",
    "\n",
    "def load_text_from_file(path: str) -> str:\n",
    "    path = path.strip().strip('\"').strip(\"'\")\n",
    "    ext = os.path.splitext(path)[1].lower()\n",
    "\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Файл не найден: {path}\")\n",
    "\n",
    "    # TXT\n",
    "    if ext == \".txt\":\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "\n",
    "    # DOCX\n",
    "    elif ext == \".docx\":\n",
    "        from docx import Document\n",
    "        doc = Document(path)\n",
    "        return \"\\n\".join(p.text for p in doc.paragraphs)\n",
    "\n",
    "    # PDF\n",
    "    elif ext == \".pdf\":\n",
    "        import pdfplumber\n",
    "        text = \"\"\n",
    "        with pdfplumber.open(path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                extracted = page.extract_text()\n",
    "                if extracted:\n",
    "                    text += extracted + \"\\n\"\n",
    "        if not text.strip():\n",
    "            raise ValueError(\"Не удалось извлечь текст из PDF\")\n",
    "        return text\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Неподдерживаемый формат файла: {ext}. \"\n",
    "            f\"Используй .txt, .docx или .pdf.\"\n",
    "        )\n",
    "\n",
    "def get_text(source_name: str):\n",
    "    while True:\n",
    "        mode = input(\n",
    "            f\"Как получить {source_name}? \"\n",
    "            f\"(1 — ввести вручную, 2 — выбрать файл, команда esc - отмена): \"\n",
    "        ).strip()\n",
    "\n",
    "        # Отмена\n",
    "        if mode.lower() == \"esc\":\n",
    "            print(\"Операция отменена пользователем.\")\n",
    "            return None, None\n",
    "\n",
    "        # Ввод текста вручную\n",
    "        if mode == \"1\":\n",
    "            print(f\"\\nВведи {source_name}:\")\n",
    "            text = input()\n",
    "\n",
    "            if text.lower() == \"esc\":\n",
    "                print(\"Операция отменена.\")\n",
    "                return None, None\n",
    "\n",
    "            preview = text.replace(\"\\n\", \" \")[:1000]\n",
    "            desc = f\"ручной ввод (первые 1000 символов: \\\"{preview}\\\")\"\n",
    "            return text, desc\n",
    "\n",
    "        # Выбор файла из проводника\n",
    "        elif mode == \"2\":\n",
    "            print(\"Откроется окно выбора файла...\")\n",
    "\n",
    "            from tkinter import Tk\n",
    "            from tkinter.filedialog import askopenfilename\n",
    "\n",
    "            Tk().withdraw()\n",
    "\n",
    "            file_path = askopenfilename(\n",
    "                title=f\"Выбор файла для {source_name}\",\n",
    "                filetypes=[\n",
    "                    (\"Все файлы\", \"*.*\")\n",
    "                    (\"Текстовые файлы\", \"*.txt\"),\n",
    "                    (\"Документы Word\", \"*.docx\"),\n",
    "                    (\"PDF файлы\", \"*.pdf\")\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            if not file_path:\n",
    "                print(\"Операция отменена.\")\n",
    "                return None, None\n",
    "\n",
    "            try:\n",
    "                text = load_text_from_file(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка при чтении файла: {e}\")\n",
    "                continue\n",
    "\n",
    "            desc = f\"файл: {file_path}\"\n",
    "            print(f\"Файл выбран: {file_path}\")\n",
    "            return text, desc\n",
    "\n",
    "        else:\n",
    "            print(\"Некорректный выбор. Введите 1, 2 или команду esc.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
