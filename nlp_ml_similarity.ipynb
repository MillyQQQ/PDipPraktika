{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "331ba65c",
   "metadata": {},
   "source": [
    "# Анализ схожести научных текстов с помощью методов естественной обработки языка и машинного обучения\n",
    "## Береза Анастасия \n",
    "## Учебная группа о.ИЗДтс 23.2/Б3-22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a7e5dd",
   "metadata": {},
   "source": [
    "## Установка зависимостей\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b54c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# При первом запуске раскомментировать строку ниже\n",
    "\n",
    "# %pip install tensorflow kagglehub pandas numpy scikit-learn joblib tqdm psutil nltk pymorphy3 matplotlib torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c2bacc",
   "metadata": {},
   "source": [
    "## Импорт библиотек\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7507c9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import tee\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "import pymorphy3\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ec766fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используемое устройство: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Используемое устройство:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b915e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.1+cu126\n",
      "12.6\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463f08a9",
   "metadata": {},
   "source": [
    "## Загрузка датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fa139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"ergkerg/russian-scientific-articles\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# Копирование в папку проекта\n",
    "\n",
    "destination_path = os.path.join(os.getcwd(), \"russian-scientific-articles\")\n",
    "shutil.copytree(path, destination_path, dirs_exist_ok=True)\n",
    "\n",
    "print(\"Файлы датасета перенесены в:\", destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f02225b",
   "metadata": {},
   "source": [
    "## Анализ набора данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae4c5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сбор датасета из txt-файлов\n",
    "\n",
    "rows = []\n",
    "for root in [\"russian-scientific-articles/data_3_1\", \"russian-scientific-articles/data_3\"]:\n",
    "     for p in Path(root).rglob(\"*.txt\"):\n",
    "        rows.append({\n",
    "              \"category\": p.parent.name,\n",
    "              \"file\": str(p),\n",
    "              \"text\": p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef94b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50d2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b27d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608ff11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af1c49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(['text',], inplace=True)\n",
    "df['text'].describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25ac7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Извлечение слов из всех статей\n",
    "\n",
    "def raw_tokens(text):\n",
    "    return re.findall(r\"\\w+\", str(text).lower())\n",
    "\n",
    "tokens = []\n",
    "for article in df[\"text\"].dropna():\n",
    "    tokens.extend(raw_tokens(article))\n",
    "\n",
    "# Гистограмма для топ-100 слов\n",
    "\n",
    "top_n = 100\n",
    "freq = Counter(tokens).most_common(top_n)\n",
    "words, counts = zip(*freq)\n",
    "\n",
    "plt.figure(figsize=(25,4))\n",
    "plt.bar(words, counts)\n",
    "plt.xticks(rotation=70, ha=\"right\")\n",
    "plt.title(\"Топ-100 слов\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Топ-100 слов:\\n\")\n",
    "for i, (w, c) in enumerate(zip(words, counts), start=1):\n",
    "    print(f\"{i:3}. {w:<20} {c}\")\n",
    "\n",
    "# Топ пар из 2 слов \n",
    "\n",
    "def bigrams(seq):\n",
    "    a, b = tee(seq)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "\n",
    "bigram_freq = Counter(bigrams(tokens)).most_common(50)\n",
    "print(\"\\nТоп 50-биграмм\")\n",
    "for (w1, w2), c in bigram_freq:\n",
    "    print(f\"{w1} {w2}: {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ff843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стоп-слова\n",
    "\n",
    "download(\"stopwords\", quiet=True)\n",
    "stop_ru = set(stopwords.words(\"russian\"))\n",
    "stop_en = set(stopwords.words(\"english\"))\n",
    "custom_stop = {\n",
    "    # указатели, ссылки\n",
    "    \"doi\",\"org\",\"orcid\",\"http\",\"https\",\"url\",\"удк\", \n",
    "    # предлоги\n",
    "    \"в\",\"и\",\"с\",\"на\",\"по\",\"для\",\"что\",\"как\",\"от\",\"из\",\"при\",\"но\",\"же\",\"у\",\"о\",\"к\",\n",
    "    # предлоги на англ\n",
    "    \"the\",\"of\",\"in\",\"on\",\"and\",\"a\",\"to\",\"for\",\"is\",\"are\",\n",
    "    # одиночные буквы\n",
    "    \"к\",\"р\",\"п\",\"г\",\"т\",\"е\",\"а\",\"м\",\"н\",\"л\",\"у\",\n",
    "    # цифры\n",
    "    \"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\"\n",
    "}\n",
    "custom_bigram_stop = {\n",
    "    # служебные\n",
    "    \"и в\",\"а также\",\"при этом\",\"а в\",\"на основе\",\"в том\",\"и др\",\"в качестве\",\"так и\",\"что в\",\"не только\",\n",
    "    \"но и\", \"таким образом\",\"том что\",\"том числе\",\"в рамках\",\"в результате\",\"в россии\",\"в виде\",\"в этом\",\n",
    "    \"с помощью\",\"на рис\",\n",
    "    # пары чисел\n",
    "    \"1 1\",\"1 2\",\"1 0\",\"2 1\",\"2 2\",\"2 3\",\"2 0\",\n",
    "    \"0 0\",\"0 1\",\"0 5\",\"а а\",\"в в\",\"в а\",\n",
    "}\n",
    "stop_all = stop_ru | stop_en | custom_stop | custom_bigram_stop\n",
    "\n",
    "morph = pymorphy3.MorphAnalyzer()\n",
    "english_stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "keep_english = False\n",
    "min_len = 3\n",
    "\n",
    "# Сохранять кэш слов\n",
    "\n",
    "lemma_cache: Dict[str, str] = {}\n",
    "stem_cache: Dict[str, str] = {}\n",
    "\n",
    "# Наборы шаблон-выражений\n",
    "\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\", re.IGNORECASE)\n",
    "EMAIL_RE = re.compile(r\"\\b[\\w.+-]+@[\\w-]+\\.[\\w.-]+\\b\")\n",
    "DOI_RE = re.compile(r\"\\b10\\.\\d{4,9}/[-._;()/:A-Za-z0-9]+\\b\", re.IGNORECASE)\n",
    "ORCID_RE = re.compile(r\"\\b\\d{4}-\\d{4}-\\d{4}-\\d{3}[0-9X]\\b\")\n",
    "NUMBER_RE = re.compile(r\"\\b\\d[\\d.,/%:-]*\\d\\b\")\n",
    "TOKEN_RE = re.compile(r\"[A-Za-z\\u0400-\\u04FF]+\")\n",
    "CYRILLIC_RE = re.compile(r\"[\\u0400-\\u04FF]\")\n",
    "LATIN_RE = re.compile(r\"[A-Za-z]\")\n",
    "\n",
    "# Функция для очистки по шаблонам\n",
    "\n",
    "def strip_noise(text: str) -> str:\n",
    "    text = text.replace(\"\\u00a0\", \" \" ).replace(\"\\ufeff\", \" \" )\n",
    "    text = URL_RE.sub(\" \" , text)\n",
    "    text = EMAIL_RE.sub(\" \" , text)\n",
    "    text = DOI_RE.sub(\" \" , text)\n",
    "    text = ORCID_RE.sub(\" \" , text)\n",
    "    text = NUMBER_RE.sub(\" \" , text)\n",
    "    return text\n",
    "\n",
    "# Функция предобработки текста \n",
    "\n",
    "def clean_text(raw: str) -> str:\n",
    "    text = strip_noise(raw.lower())\n",
    "    lemmas: list[str] = []\n",
    "    for token in TOKEN_RE.findall(text):\n",
    "        if len(token) < min_len:\n",
    "            continue\n",
    "        has_cyr = bool(CYRILLIC_RE.search(token))\n",
    "        has_lat = bool(LATIN_RE.search(token))\n",
    "        if has_cyr and has_lat:\n",
    "            continue\n",
    "        if has_cyr:\n",
    "            lemma = lemma_cache.get(token)\n",
    "            if lemma is None:\n",
    "                lemma = morph.parse(token)[0].normal_form\n",
    "                lemma_cache[token] = lemma\n",
    "            if len(lemma) >= min_len and lemma not in stop_all:\n",
    "                lemmas.append(lemma)\n",
    "        elif has_lat and keep_english:\n",
    "            stem = stem_cache.get(token)\n",
    "            if stem is None:\n",
    "                stem = english_stemmer.stem(token)\n",
    "                stem_cache[token] = stem\n",
    "            if len(stem) >= min_len and stem not in stop_all:\n",
    "                lemmas.append(stem)\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "# Предобработка датасета с помощью функции \n",
    "\n",
    "clean_df = (\n",
    "    df.assign(clean_text=df[\"text\"].map(clean_text))\n",
    "      .loc[lambda d: d[\"clean_text\"] != \"\"]\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "clean_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6929fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b4f3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df[\"clean_text\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be21623d",
   "metadata": {},
   "source": [
    "### Визуализация частоты использования очищенных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3818b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = clean_df[\"clean_text\"].str.split().explode()\n",
    "freq = tokens.value_counts().head(30)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "freq.sort_values().plot(kind=\"barh\")\n",
    "plt.xlabel(\"Частота\")\n",
    "plt.ylabel(\"Слова\")\n",
    "plt.title(\"Топ-30 самых частых лемм после очистки\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5550cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение в файл очищенный набор данных\n",
    "clean_df.to_json(\"cleaned_dataset.jsonl\", orient=\"records\", lines=True, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab9c966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Количество статей по рубрикам\n",
    "category_counts = (clean_df[\"category\"].value_counts().sort_values())\n",
    "\n",
    "# Вывод столбчатой диаграммы\n",
    "fig, ax = plt.subplots(figsize=(25, 5))\n",
    "ax.bar(category_counts.index.astype(str), category_counts.values)\n",
    "ax.set_xlabel(\"Рубрика\")\n",
    "ax.set_ylabel(\"Количество статей\")\n",
    "ax.set_title(\"Распределение статей по рубрикам\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c95bfcde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>file</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 Автоматика. Вычислительная техника</td>\n",
       "      <td>russian-scientific-articles\\data_3_1\\1 Автомат...</td>\n",
       "      <td>﻿2011 Компьютерная оптика, том 35, № 2 \\n\\nАЛГ...</td>\n",
       "      <td>компьютерный оптика алгоритм встраивание полух...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 Автоматика. Вычислительная техника</td>\n",
       "      <td>russian-scientific-articles\\data_3_1\\1 Автомат...</td>\n",
       "      <td>﻿Software &amp; Systems                        no....</td>\n",
       "      <td>программный продукт система дата подача статья...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 Автоматика. Вычислительная техника</td>\n",
       "      <td>russian-scientific-articles\\data_3_1\\1 Автомат...</td>\n",
       "      <td>﻿Выделение контуров на изображениях с помощью ...</td>\n",
       "      <td>выделение контур изображение помощь алгоритм к...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 Автоматика. Вычислительная техника</td>\n",
       "      <td>russian-scientific-articles\\data_3_1\\1 Автомат...</td>\n",
       "      <td>﻿Программные продукты и системы / Software &amp; S...</td>\n",
       "      <td>программный продукт система дата подача статья...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 Автоматика. Вычислительная техника</td>\n",
       "      <td>russian-scientific-articles\\data_3_1\\1 Автомат...</td>\n",
       "      <td>﻿Алгоритм поэтапного уточнения проективного пр...</td>\n",
       "      <td>алгоритм поэтапный уточнение проективный преоб...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               category  \\\n",
       "0  1 Автоматика. Вычислительная техника   \n",
       "1  1 Автоматика. Вычислительная техника   \n",
       "2  1 Автоматика. Вычислительная техника   \n",
       "3  1 Автоматика. Вычислительная техника   \n",
       "4  1 Автоматика. Вычислительная техника   \n",
       "\n",
       "                                                file  \\\n",
       "0  russian-scientific-articles\\data_3_1\\1 Автомат...   \n",
       "1  russian-scientific-articles\\data_3_1\\1 Автомат...   \n",
       "2  russian-scientific-articles\\data_3_1\\1 Автомат...   \n",
       "3  russian-scientific-articles\\data_3_1\\1 Автомат...   \n",
       "4  russian-scientific-articles\\data_3_1\\1 Автомат...   \n",
       "\n",
       "                                                text  \\\n",
       "0  ﻿2011 Компьютерная оптика, том 35, № 2 \\n\\nАЛГ...   \n",
       "1  ﻿Software & Systems                        no....   \n",
       "2  ﻿Выделение контуров на изображениях с помощью ...   \n",
       "3  ﻿Программные продукты и системы / Software & S...   \n",
       "4  ﻿Алгоритм поэтапного уточнения проективного пр...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  компьютерный оптика алгоритм встраивание полух...  \n",
       "1  программный продукт система дата подача статья...  \n",
       "2  выделение контур изображение помощь алгоритм к...  \n",
       "3  программный продукт система дата подача статья...  \n",
       "4  алгоритм поэтапный уточнение проективный преоб...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка набора данных из файла\n",
    "clean_df = pd.read_json(\"cleaned_dataset.jsonl\", lines=True)\n",
    "clean_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d42d53",
   "metadata": {},
   "source": [
    "# Обучение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cdccff",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ee84059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер train: (1952, 4)\n",
      "Размер test: (488, 4)\n"
     ]
    }
   ],
   "source": [
    "# Разделение на обучающую и валидационную выборку\n",
    "\n",
    "train_df, test_df = train_test_split(clean_df, test_size=0.2, stratify=clean_df[\"category\"], random_state=42)\n",
    "\n",
    "print(\"Размер train:\", train_df.shape)\n",
    "print(\"Размер test:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdae0ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Матрица X_train: (1952, 30000)\n",
      "Матрица X_test: (488, 30000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Векторизация TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=30000, ngram_range=(1, 2), lowercase=False, min_df=2)\n",
    "\n",
    "train_texts = train_df[\"clean_text\"].tolist()\n",
    "test_texts = test_df[\"clean_text\"].tolist()\n",
    "\n",
    "X_train = vectorizer.fit_transform(train_texts)\n",
    "X_test = vectorizer.transform(test_texts)\n",
    "\n",
    "y_train = train_df[\"category\"].astype(str).to_numpy()\n",
    "y_test = test_df[\"category\"].astype(str).to_numpy()\n",
    "\n",
    "print(\"Матрица X_train:\", X_train.shape)\n",
    "print(\"Матрица X_test:\", X_test.shape)\n",
    "\n",
    "# Сохранение векторизатора для будущего использования\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d727f256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNZ train: 1958084\n",
      "Среднее число ненулевых признаков на документ: 1003.1168032786885\n"
     ]
    }
   ],
   "source": [
    "print(\"NNZ train:\", X_train.nnz)\n",
    "print(\"Среднее число ненулевых признаков на документ:\",\n",
    "      X_train.nnz / X_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dea367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция создания пар текстов\n",
    "\n",
    "def make_pairs(X, y, n_pos=3, n_neg=3, random_state=42):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    # Индексы документов по категориям\n",
    "\n",
    "    label2idx = defaultdict(list)\n",
    "    for idx, label in enumerate(y):\n",
    "        label2idx[label].append(idx)\n",
    "\n",
    "    all_index = np.arange(len(y))\n",
    "    pair_i = []\n",
    "    pair_j = []\n",
    "    pair_labels = []\n",
    "\n",
    "    for label, idxs in label2idx.items():\n",
    "        idxs = np.asarray(idxs)\n",
    "        other_index = np.setdiff1d(all_index, idxs)\n",
    "\n",
    "        for i in idxs:\n",
    "            if len(idxs) > 1:\n",
    "                pos_candidates = idxs[idxs != i]\n",
    "                n_sample_pos = min(n_pos, len(pos_candidates))\n",
    "                pos = rng.choice(pos_candidates, size=n_sample_pos, replace=False)\n",
    "                for j in pos:\n",
    "                    pair_i.append(i)\n",
    "                    pair_j.append(j)\n",
    "                    pair_labels.append(1)\n",
    "            if len(other_index) > 0:\n",
    "                n_sample_neg = min(n_neg, len(other_index))\n",
    "                neg = rng.choice(other_index, size=n_sample_neg, replace=False)\n",
    "                for j in neg:\n",
    "                    pair_i.append(i)\n",
    "                    pair_j.append(j)\n",
    "                    pair_labels.append(0)\n",
    "\n",
    "    pair_i = np.asarray(pair_i)\n",
    "    pair_j = np.asarray(pair_j)\n",
    "    pair_labels = np.asarray(pair_labels, dtype=int)\n",
    "\n",
    "    print(f\"Сформировано пар: {len(pair_labels)} \"\n",
    "          f\"(положительных: {(pair_labels == 1).sum()}, отрицательных: {(pair_labels == 0).sum()})\")\n",
    "\n",
    "    return pair_i, pair_j, pair_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf776c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сформировано пар: 11712 (положительных: 5856, отрицательных: 5856)\n",
      "Сформировано пар: 2925 (положительных: 1461, отрицательных: 1464)\n"
     ]
    }
   ],
   "source": [
    "# Формирование обучающих данных для модели\n",
    "\n",
    "train_pairs = make_pairs(X_train, y_train, n_pos=3, n_neg=3, random_state=42)\n",
    "test_pairs  = make_pairs(X_test,  y_test,  n_pos=3, n_neg=3, random_state=43)\n",
    "\n",
    "train_idx_i, train_idx_j, y_train_pairs = train_pairs\n",
    "test_idx_i,  test_idx_j,  y_test_pairs  = test_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40648ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Вычисление сходства с помощью косинуса для каждой пары\n",
    "\n",
    "def pair_cosine_features(X, idx_i, idx_j):\n",
    "    idx_i = np.asarray(idx_i)\n",
    "    idx_j = np.asarray(idx_j)\n",
    "    sims = []\n",
    "    for one_i, one_j in zip(idx_i, idx_j):\n",
    "        Xi = X[one_i]\n",
    "        Xj = X[one_j]\n",
    "        cos_value = cosine_similarity(Xi, Xj)[0, 0]\n",
    "        sims.append([cos_value])\n",
    "    sims_array = np.array(sims)\n",
    "    return sims_array\n",
    "\n",
    "# Косинусное сходство всех пар для наборов\n",
    "\n",
    "X_train_pairs = pair_cosine_features(X_train, train_idx_i, train_idx_j)\n",
    "X_test_pairs = pair_cosine_features(X_test, test_idx_i, test_idx_j)\n",
    "\n",
    "print(\"Размер X_train_pairs:\", X_train_pairs.shape)\n",
    "print(\"Размер X_test_pairs:\", X_test_pairs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
